import json
import pandas as pd
import tensorflow as tf
from transformers import TFDistilBertModel, DistilBertTokenizerFast
from .preprocess import encode

class Model:
    def __init__(self, padding: int):

        """
            Initialize the model class
            Inputs:
            - padding:            Maximum number of encoded tokens in a given sequence 
        """

        self.padding = padding

    def build_model(self, model_name: str='distilbert-base-uncased', random_seed: int = 42):
        """
            Builds the intent and sequence classification model with keras' model API.
            Inputs:
             - model_name:  The huggingface transformer to use.  Distilbert-base-uncased is recommended
             - random_seed: The selected random seed for reproducible results
            Outputs:
             - saves the uncompiled model to the class
        """

        # define the tokenizer
        self.tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)

        # build the nlp model
        weight_initializer = tf.keras.initializers.GlorotNormal(seed=random_seed)

        #* inputs
        input_ids_layer       = tf.keras.layers.Input(shape=(self.padding), name='input_ids',       dtype='int32')
        input_attention_layer = tf.keras.layers.Input(shape=(self.padding), name='input_attention', dtype='int32')

        #* transformer
        transformer = TFDistilBertModel.from_pretrained(model_name)
        last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0] # (batch_size, sequence_length, hidden_size=768)

        #* outputs
        cls_token = last_hidden_state[:, 0, :]

        intent_output = tf.keras.layers.Dense(
            2,
            activation='softmax',
            kernel_initializer=weight_initializer,
            kernel_constraint=None,
            bias_initializer='zeros',
            name='intent'
        )(cls_token)

        # define the model
        self.model = tf.keras.Model([input_ids_layer, input_attention_layer], intent_output)
        self.model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5),
                           loss      = tf.keras.losses.CategoricalCrossentropy(),
                           metrics   = tf.keras.metrics.CategoricalAccuracy('categorical_accuracy'))
    
    def train(self, dataset_path: str, epochs: int, batch_size: int = 16):
        """
            Train the model on a dataset generated by the data_processing.generate_dataset function
            Inputs:
             - dataset_path: path to the dataset .pkl file
             - epochs:       number of epochs to train for
             - batch_size:   model batch size
        """

        # load in the dataset
        df_train = pd.read_pickle(dataset_path)

        # prepare model inputs
        x_train_ids, x_train_attention = encode(
            self.tokenizer, 
            list(df_train['contract']), 
            max_len=self.padding
        )

        # prepare model outputs
        y_train_intents = tf.one_hot(df_train['label'].values, 2)

        # train the model
        history = self.model.fit(
            x = [x_train_ids, x_train_attention],
            y = y_train_intents,
            epochs = epochs,
            batch_size = batch_size,
            verbose = 1
        )
        
        # print(f'{json.dumps(history.history)}')

    def load_model(self, model_path: str, model_name: str):
        """
            Load in a pre-trained model
            Inputs:
             - model_path: Path to the weights of the pre-trained model
             - model_name: The huggingface transformer to use.  Distilbert-base-uncased is recommended
             
        """

        #? errors will arise if the pre-trained weights do not exist
        self.build_model(model_name)
        self.model.load_weights(model_path)

    def save_model(self, model_path: str):
        """
            Save the model's weights to a directory
            Inputs:
             - model_path: directory to save the weights in
        """

        self.model.save_weights(model_path)

    def classify(self, text: str, max_len: int) -> object:
        """
            Classify a single text prompt
            Inputs:
             - text: prompt to classify
             - max_len: encoded length of a prompt
            Outputs:
             - info: json object containing decoded predictions (entities & intents)
        """

        input = encode(self.tokenizer, text, max_len)
        model_output = self.model(input)

        info = model_output[0]
        
        return info